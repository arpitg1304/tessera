{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Tessera: Generate Embeddings from LeRobot Datasets\n\nThis notebook generates CLIP embeddings from LeRobot v3.0 datasets for visualization in [Tessera](https://github.com/arpitg1304/tessera).\n\n**Features:**\n- Generate CLIP embeddings from any LeRobot dataset on HuggingFace\n- Optional thumbnail and GIF previews for hover visualization\n- Configurable embedding modes (single frame or start+end)\n- Export to HDF5 format compatible with Tessera\n\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1pVsTizT8Ec1iST0tyNDzhgh4h6YUtZUh)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q torch torchvision\n",
    "!pip install -q git+https://github.com/openai/CLIP.git\n",
    "!pip install -q h5py pillow pandas pyarrow av huggingface_hub tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify GPU availability\n",
    "import torch\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration\n",
    "\n",
    "Configure your embedding generation settings below. You can use the interactive widgets or modify the values directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CONFIGURATION - Modify these values or use the interactive widgets below\n",
    "# =============================================================================\n",
    "\n",
    "# Dataset settings\n",
    "DATASET_NAME = \"lerobot/pusht\"  # @param {type:\"string\"}\n",
    "MAX_EPISODES = 0  # @param {type:\"integer\"} 0 = all episodes\n",
    "\n",
    "# Device settings\n",
    "DEVICE = \"auto\"  # @param [\"auto\", \"cuda\", \"cpu\"]\n",
    "\n",
    "# Embedding mode\n",
    "EMBEDDING_MODE = \"start_end\"  # @param [\"single\", \"start_end\"]\n",
    "\n",
    "# Thumbnail settings\n",
    "GENERATE_THUMBNAILS = True  # @param {type:\"boolean\"}\n",
    "THUMBNAIL_QUALITY = \"medium\"  # @param [\"low\", \"medium\", \"high\"]\n",
    "\n",
    "# GIF settings\n",
    "GENERATE_GIFS = True  # @param {type:\"boolean\"}\n",
    "GIF_QUALITY = \"medium\"  # @param [\"low\", \"medium\", \"high\"]\n",
    "\n",
    "# Output settings\n",
    "OUTPUT_FILENAME = \"\"  # @param {type:\"string\"} Leave empty for auto-generated name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quality presets\n",
    "THUMBNAIL_PRESETS = {\n",
    "    \"low\": {\"size\": (64, 64), \"quality\": 60},\n",
    "    \"medium\": {\"size\": (128, 128), \"quality\": 80},\n",
    "    \"high\": {\"size\": (192, 192), \"quality\": 90}\n",
    "}\n",
    "\n",
    "GIF_PRESETS = {\n",
    "    \"low\": {\"size\": (64, 64), \"fps\": 6, \"max_frames\": 8},\n",
    "    \"medium\": {\"size\": (128, 128), \"fps\": 8, \"max_frames\": 16},\n",
    "    \"high\": {\"size\": (192, 192), \"fps\": 10, \"max_frames\": 24}\n",
    "}\n",
    "\n",
    "# Apply presets\n",
    "thumb_config = THUMBNAIL_PRESETS[THUMBNAIL_QUALITY]\n",
    "gif_config = GIF_PRESETS[GIF_QUALITY]\n",
    "\n",
    "# Auto-detect device\n",
    "if DEVICE == \"auto\":\n",
    "    DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Generate output filename if not specified\n",
    "if not OUTPUT_FILENAME:\n",
    "    dataset_short_name = DATASET_NAME.split(\"/\")[-1]\n",
    "    suffix = \"_gifs\" if GENERATE_GIFS else (\"_thumbs\" if GENERATE_THUMBNAILS else \"\")\n",
    "    OUTPUT_FILENAME = f\"{dataset_short_name}_embeddings{suffix}.h5\"\n",
    "\n",
    "print(\"Configuration:\")\n",
    "print(f\"  Dataset: {DATASET_NAME}\")\n",
    "print(f\"  Device: {DEVICE}\")\n",
    "print(f\"  Embedding mode: {EMBEDDING_MODE}\")\n",
    "print(f\"  Thumbnails: {GENERATE_THUMBNAILS} ({THUMBNAIL_QUALITY})\")\n",
    "print(f\"  GIFs: {GENERATE_GIFS} ({GIF_QUALITY})\")\n",
    "print(f\"  Output: {OUTPUT_FILENAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Download Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import snapshot_download\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "# Download dataset\n",
    "print(f\"Downloading {DATASET_NAME}...\")\n",
    "dataset_path = Path(snapshot_download(\n",
    "    repo_id=DATASET_NAME,\n",
    "    repo_type=\"dataset\",\n",
    "    local_dir=f\"./datasets/{DATASET_NAME.replace('/', '_')}\"\n",
    "))\n",
    "print(f\"Downloaded to: {dataset_path}\")\n",
    "\n",
    "# Load dataset info\n",
    "info_path = dataset_path / \"meta\" / \"info.json\"\n",
    "with open(info_path) as f:\n",
    "    dataset_info = json.load(f)\n",
    "\n",
    "print(f\"\\nDataset info:\")\n",
    "print(f\"  Total episodes: {dataset_info.get('total_episodes', 'unknown')}\")\n",
    "print(f\"  Total frames: {dataset_info.get('total_frames', 'unknown')}\")\n",
    "print(f\"  FPS: {dataset_info.get('fps', 'unknown')}\")\n",
    "print(f\"  Video keys: {list(dataset_info.get('video_keys', []))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load CLIP Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import clip\n",
    "\n",
    "print(f\"Loading CLIP model on {DEVICE}...\")\n",
    "clip_model, clip_preprocess = clip.load(\"ViT-B/32\", device=DEVICE)\n",
    "clip_model.eval()\n",
    "print(\"CLIP model loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import av\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "def extract_frame_from_video(video_path: str, frame_idx: int) -> np.ndarray:\n",
    "    \"\"\"Extract a single frame from a video file.\"\"\"\n",
    "    container = av.open(str(video_path))\n",
    "    stream = container.streams.video[0]\n",
    "    \n",
    "    # Seek to approximate position\n",
    "    target_pts = int(frame_idx * stream.duration / stream.frames) if stream.frames else 0\n",
    "    container.seek(target_pts, stream=stream)\n",
    "    \n",
    "    current_frame = 0\n",
    "    for frame in container.decode(video=0):\n",
    "        if current_frame >= frame_idx:\n",
    "            container.close()\n",
    "            return frame.to_ndarray(format='rgb24')\n",
    "        current_frame += 1\n",
    "    \n",
    "    container.close()\n",
    "    return None\n",
    "\n",
    "\n",
    "def extract_frames_for_gif(video_path: str, start_frame: int, end_frame: int, \n",
    "                           max_frames: int = 16) -> list:\n",
    "    \"\"\"Extract evenly spaced frames for GIF creation.\"\"\"\n",
    "    total_frames = end_frame - start_frame\n",
    "    if total_frames <= max_frames:\n",
    "        frame_indices = list(range(start_frame, end_frame))\n",
    "    else:\n",
    "        frame_indices = np.linspace(start_frame, end_frame - 1, max_frames, dtype=int).tolist()\n",
    "    \n",
    "    frames = []\n",
    "    container = av.open(str(video_path))\n",
    "    \n",
    "    current_frame = 0\n",
    "    for frame in container.decode(video=0):\n",
    "        if current_frame in frame_indices:\n",
    "            frames.append(frame.to_ndarray(format='rgb24'))\n",
    "        if current_frame > max(frame_indices):\n",
    "            break\n",
    "        current_frame += 1\n",
    "    \n",
    "    container.close()\n",
    "    return frames\n",
    "\n",
    "\n",
    "def create_thumbnail(image: np.ndarray, size: tuple, quality: int) -> bytes:\n",
    "    \"\"\"Create a JPEG thumbnail from an image.\"\"\"\n",
    "    pil_image = Image.fromarray(image)\n",
    "    pil_image = pil_image.resize(size, Image.LANCZOS)\n",
    "    buffer = io.BytesIO()\n",
    "    pil_image.save(buffer, format='JPEG', quality=quality)\n",
    "    return buffer.getvalue()\n",
    "\n",
    "\n",
    "def create_gif(frames: list, size: tuple, fps: int) -> bytes:\n",
    "    \"\"\"Create an animated GIF from frames.\"\"\"\n",
    "    pil_frames = [Image.fromarray(f).resize(size, Image.LANCZOS) for f in frames]\n",
    "    \n",
    "    buffer = io.BytesIO()\n",
    "    pil_frames[0].save(\n",
    "        buffer,\n",
    "        format='GIF',\n",
    "        save_all=True,\n",
    "        append_images=pil_frames[1:],\n",
    "        duration=int(1000 / fps),\n",
    "        loop=0\n",
    "    )\n",
    "    return buffer.getvalue()\n",
    "\n",
    "\n",
    "def get_clip_embedding(image: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Generate CLIP embedding for an image.\"\"\"\n",
    "    pil_image = Image.fromarray(image)\n",
    "    image_input = clip_preprocess(pil_image).unsqueeze(0).to(DEVICE)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        features = clip_model.encode_image(image_input)\n",
    "        features = features / features.norm(dim=-1, keepdim=True)\n",
    "    \n",
    "    return features.cpu().numpy().flatten()\n",
    "\n",
    "print(\"Helper functions loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Parse Dataset Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import pandas as pd\nfrom pathlib import Path\n\n# Find video key to use - handle different LeRobot info.json formats\nvideo_keys = dataset_info.get('video_keys', [])\n\n# If video_keys not present, extract from features (newer format)\nif not video_keys and 'features' in dataset_info:\n    features = dataset_info['features']\n    video_keys = [k for k, v in features.items() \n                  if isinstance(v, dict) and v.get('dtype') == 'video']\n    print(f\"Extracted video keys from features: {video_keys}\")\n\n# Preferred video keys to try\npreferred_keys = [\n    'observation.images.top', \n    'observation.image', \n    'observation.images.wrist_image_left',\n    'observation.images.exterior_image_1_left',\n    'observation.images.exterior_image_2_left'\n]\n\nvideo_key = None\nfor key in preferred_keys:\n    if key in video_keys:\n        video_key = key\n        break\nif video_key is None and video_keys:\n    video_key = video_keys[0]\n\nprint(f\"Available video keys: {video_keys}\")\nprint(f\"Using video key: {video_key}\")\n\n# Load episode info\nepisodes_parquet = list((dataset_path / \"meta\" / \"episodes\").glob(\"**/*.parquet\"))\nepisode_df = pd.concat([pd.read_parquet(p) for p in episodes_parquet])\nepisode_df = episode_df.sort_values('episode_index').reset_index(drop=True)\n\n# Load frame data to get episode boundaries\ndata_parquets = list((dataset_path / \"data\").glob(\"**/*.parquet\"))\nframe_df = pd.concat([pd.read_parquet(p) for p in data_parquets])\nframe_df = frame_df.sort_values(['episode_index', 'frame_index']).reset_index(drop=True)\n\n# Get episode boundaries\nepisode_info = []\nfor ep_idx in episode_df['episode_index'].unique():\n    ep_frames = frame_df[frame_df['episode_index'] == ep_idx]\n    episode_info.append({\n        'episode_index': ep_idx,\n        'start_frame': ep_frames['frame_index'].min(),\n        'end_frame': ep_frames['frame_index'].max() + 1,\n        'length': len(ep_frames)\n    })\n\nepisode_info = pd.DataFrame(episode_info)\ntotal_episodes = len(episode_info)\n\n# Apply max episodes limit\nif MAX_EPISODES > 0 and MAX_EPISODES < total_episodes:\n    episode_info = episode_info.head(MAX_EPISODES)\n    print(f\"Limited to {MAX_EPISODES} episodes (out of {total_episodes})\")\nelse:\n    print(f\"Processing all {total_episodes} episodes\")\n\nprint(f\"\\nEpisode stats:\")\nprint(f\"  Min length: {episode_info['length'].min()} frames\")\nprint(f\"  Max length: {episode_info['length'].max()} frames\")\nprint(f\"  Mean length: {episode_info['length'].mean():.1f} frames\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Find Video Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Find video files for the selected video key\nif video_key is None:\n    raise ValueError(\n        f\"No video keys found in dataset! \"\n        f\"Available keys in info.json: {list(dataset_info.keys())}\\n\"\n        f\"This dataset may not contain video data.\"\n    )\n\n# Try different path formats (LeRobot v3 uses nested structure)\nvideo_dir = dataset_path / \"videos\" / video_key.replace('.', '/')\nif not video_dir.exists():\n    # Try with dots preserved\n    video_dir = dataset_path / \"videos\" / video_key\nif not video_dir.exists():\n    # Try finding any videos directory\n    videos_base = dataset_path / \"videos\"\n    if videos_base.exists():\n        # List available video directories\n        available = list(videos_base.glob(\"*\"))\n        print(f\"Video key '{video_key}' not found. Available directories:\")\n        for d in available[:5]:\n            print(f\"  {d.name}\")\n        raise ValueError(f\"Could not find video directory for key: {video_key}\")\n    else:\n        raise ValueError(f\"No 'videos' directory found in dataset at {dataset_path}\")\n\nvideo_files = sorted(video_dir.glob(\"**/*.mp4\"))\n\nif len(video_files) == 0:\n    raise ValueError(f\"No .mp4 files found in {video_dir}\")\n\nprint(f\"Found {len(video_files)} video file(s) in {video_dir.name}\")\nfor vf in video_files[:3]:\n    print(f\"  {vf.name}\")\nif len(video_files) > 3:\n    print(f\"  ... and {len(video_files) - 3} more\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Generate Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storage for results\n",
    "embeddings = []\n",
    "episode_ids = []\n",
    "thumbnails = [] if GENERATE_THUMBNAILS else None\n",
    "gifs = [] if GENERATE_GIFS else None\n",
    "metadata = {\n",
    "    'episode_length': []\n",
    "}\n",
    "\n",
    "# For LeRobot v3, videos are chunked - we need to map episodes to video files\n",
    "# Typically: chunk-000/file-000.mp4 contains frames for multiple episodes\n",
    "\n",
    "# Build a mapping of global frame index to video file\n",
    "# For simplicity, if there's only one video file, use it for all\n",
    "if len(video_files) == 1:\n",
    "    video_path = video_files[0]\n",
    "    print(f\"Using single video file: {video_path.name}\")\n",
    "else:\n",
    "    # Multiple video files - need to handle chunking\n",
    "    video_path = video_files[0]  # Simplified - may need adjustment for your dataset\n",
    "    print(f\"Warning: Multiple video files found, using first: {video_path.name}\")\n",
    "\n",
    "print(f\"\\nGenerating embeddings...\")\n",
    "print(f\"  Mode: {EMBEDDING_MODE}\")\n",
    "print(f\"  Thumbnails: {GENERATE_THUMBNAILS}\")\n",
    "print(f\"  GIFs: {GENERATE_GIFS}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process each episode\n",
    "for idx, row in tqdm(episode_info.iterrows(), total=len(episode_info), desc=\"Processing episodes\"):\n",
    "    ep_idx = row['episode_index']\n",
    "    start_frame = row['start_frame']\n",
    "    end_frame = row['end_frame']\n",
    "    ep_length = row['length']\n",
    "    \n",
    "    # Calculate frame indices\n",
    "    middle_frame = start_frame + ep_length // 2\n",
    "    last_frame = end_frame - 1\n",
    "    \n",
    "    try:\n",
    "        # Extract frames and generate embedding based on mode\n",
    "        if EMBEDDING_MODE == \"single\":\n",
    "            frame = extract_frame_from_video(video_path, middle_frame)\n",
    "            if frame is None:\n",
    "                print(f\"Warning: Could not extract frame for episode {ep_idx}\")\n",
    "                continue\n",
    "            embedding = get_clip_embedding(frame)\n",
    "        \n",
    "        elif EMBEDDING_MODE == \"start_end\":\n",
    "            start_frame_img = extract_frame_from_video(video_path, start_frame)\n",
    "            end_frame_img = extract_frame_from_video(video_path, last_frame)\n",
    "            \n",
    "            if start_frame_img is None or end_frame_img is None:\n",
    "                print(f\"Warning: Could not extract frames for episode {ep_idx}\")\n",
    "                continue\n",
    "            \n",
    "            start_emb = get_clip_embedding(start_frame_img)\n",
    "            end_emb = get_clip_embedding(end_frame_img)\n",
    "            embedding = np.concatenate([start_emb, end_emb])\n",
    "            frame = start_frame_img  # Use start frame for thumbnail\n",
    "        \n",
    "        embeddings.append(embedding)\n",
    "        episode_ids.append(f\"episode_{ep_idx:05d}\")\n",
    "        metadata['episode_length'].append(ep_length)\n",
    "        \n",
    "        # Generate thumbnail\n",
    "        if GENERATE_THUMBNAILS:\n",
    "            thumb_bytes = create_thumbnail(\n",
    "                frame, \n",
    "                thumb_config['size'], \n",
    "                thumb_config['quality']\n",
    "            )\n",
    "            thumbnails.append(np.frombuffer(thumb_bytes, dtype=np.uint8))\n",
    "        \n",
    "        # Generate GIF\n",
    "        if GENERATE_GIFS:\n",
    "            gif_frames = extract_frames_for_gif(\n",
    "                video_path, \n",
    "                start_frame, \n",
    "                end_frame,\n",
    "                gif_config['max_frames']\n",
    "            )\n",
    "            if gif_frames:\n",
    "                gif_bytes = create_gif(\n",
    "                    gif_frames,\n",
    "                    gif_config['size'],\n",
    "                    gif_config['fps']\n",
    "                )\n",
    "                gifs.append(np.frombuffer(gif_bytes, dtype=np.uint8))\n",
    "            else:\n",
    "                # Fallback: empty GIF placeholder\n",
    "                gifs.append(np.array([], dtype=np.uint8))\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing episode {ep_idx}: {e}\")\n",
    "        continue\n",
    "\n",
    "print(f\"\\nProcessed {len(embeddings)} episodes successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Save to HDF5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "\n",
    "# Convert to arrays\n",
    "embeddings_array = np.array(embeddings, dtype=np.float32)\n",
    "\n",
    "print(f\"Saving to {OUTPUT_FILENAME}...\")\n",
    "print(f\"  Episodes: {len(embeddings)}\")\n",
    "print(f\"  Embedding dimension: {embeddings_array.shape[1]}\")\n",
    "\n",
    "with h5py.File(OUTPUT_FILENAME, 'w') as f:\n",
    "    # Required: embeddings and episode_ids\n",
    "    f.create_dataset('embeddings', data=embeddings_array, compression='gzip')\n",
    "    f.create_dataset('episode_ids', data=episode_ids)\n",
    "    \n",
    "    # Metadata\n",
    "    meta_group = f.create_group('metadata')\n",
    "    meta_group.create_dataset('episode_length', data=metadata['episode_length'])\n",
    "    \n",
    "    # Add dataset name as metadata\n",
    "    dataset_labels = [DATASET_NAME.split('/')[-1]] * len(embeddings)\n",
    "    meta_group.create_dataset('dataset', data=dataset_labels)\n",
    "    \n",
    "    # Optional: thumbnails\n",
    "    if GENERATE_THUMBNAILS and thumbnails:\n",
    "        vlen_dtype = h5py.vlen_dtype(np.uint8)\n",
    "        thumb_ds = f.create_dataset('thumbnails', (len(thumbnails),), dtype=vlen_dtype)\n",
    "        for i, thumb in enumerate(thumbnails):\n",
    "            thumb_ds[i] = thumb\n",
    "        thumb_size_mb = sum(len(t) for t in thumbnails) / 1024 / 1024\n",
    "        print(f\"  Thumbnails: {len(thumbnails)} ({thumb_size_mb:.2f} MB)\")\n",
    "    \n",
    "    # Optional: GIFs\n",
    "    if GENERATE_GIFS and gifs:\n",
    "        vlen_dtype = h5py.vlen_dtype(np.uint8)\n",
    "        gif_ds = f.create_dataset('gifs', (len(gifs),), dtype=vlen_dtype)\n",
    "        for i, gif in enumerate(gifs):\n",
    "            gif_ds[i] = gif\n",
    "        gif_size_mb = sum(len(g) for g in gifs) / 1024 / 1024\n",
    "        print(f\"  GIFs: {len(gifs)} ({gif_size_mb:.2f} MB)\")\n",
    "\n",
    "# Report file size\n",
    "file_size_mb = Path(OUTPUT_FILENAME).stat().st_size / 1024 / 1024\n",
    "print(f\"\\nOutput file: {OUTPUT_FILENAME}\")\n",
    "print(f\"File size: {file_size_mb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Verify Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the generated file\n",
    "print(\"Verifying output file...\\n\")\n",
    "\n",
    "with h5py.File(OUTPUT_FILENAME, 'r') as f:\n",
    "    print(\"File structure:\")\n",
    "    def print_structure(name, obj):\n",
    "        if isinstance(obj, h5py.Dataset):\n",
    "            print(f\"  {name}: {obj.shape} {obj.dtype}\")\n",
    "        else:\n",
    "            print(f\"  {name}/\")\n",
    "    f.visititems(print_structure)\n",
    "    \n",
    "    print(f\"\\nEmbeddings shape: {f['embeddings'].shape}\")\n",
    "    print(f\"Episode IDs: {len(f['episode_ids'])}\")\n",
    "    \n",
    "    if 'thumbnails' in f:\n",
    "        print(f\"Thumbnails: {len(f['thumbnails'])}\")\n",
    "    if 'gifs' in f:\n",
    "        print(f\"GIFs: {len(f['gifs'])}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"SUCCESS! Your embedding file is ready.\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Download File\n",
    "\n",
    "Run the cell below to download your embedding file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the file (works in Google Colab)\n",
    "try:\n",
    "    from google.colab import files\n",
    "    files.download(OUTPUT_FILENAME)\n",
    "    print(f\"Downloading {OUTPUT_FILENAME}...\")\n",
    "except ImportError:\n",
    "    print(f\"Not running in Colab. File saved to: {OUTPUT_FILENAME}\")\n",
    "    print(f\"\\nYou can download it manually or upload directly to Tessera.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Upload to Tessera (Optional)\n",
    "\n",
    "You can upload directly to Tessera from this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Upload directly to Tessera\n",
    "TESSERA_HOST = \"https://tessera.vlastudio.cloud\"  # @param {type:\"string\"}\n",
    "UPLOAD_TO_TESSERA = False  # @param {type:\"boolean\"}\n",
    "\n",
    "if UPLOAD_TO_TESSERA:\n",
    "    import requests\n",
    "    \n",
    "    print(f\"Uploading to {TESSERA_HOST}...\")\n",
    "    \n",
    "    with open(OUTPUT_FILENAME, 'rb') as f:\n",
    "        response = requests.post(\n",
    "            f\"{TESSERA_HOST}/api/upload\",\n",
    "            files={'file': (OUTPUT_FILENAME, f, 'application/x-hdf5')}\n",
    "        )\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        result = response.json()\n",
    "        project_id = result.get('project_id')\n",
    "        print(f\"\\nUpload successful!\")\n",
    "        print(f\"View your embeddings at: {TESSERA_HOST}/project/{project_id}\")\n",
    "    else:\n",
    "        print(f\"Upload failed: {response.status_code}\")\n",
    "        print(response.text)\n",
    "else:\n",
    "    print(\"Upload skipped. Set UPLOAD_TO_TESSERA = True to upload.\")\n",
    "    print(f\"\\nOr upload manually at: {TESSERA_HOST}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "1. **Upload to Tessera**: Drag and drop your `.h5` file at [tessera.vlastudio.cloud](https://tessera.vlastudio.cloud)\n",
    "2. **Explore**: Use the interactive scatter plot to explore your embeddings\n",
    "3. **Sample**: Select diverse episodes using K-means or stratified sampling\n",
    "4. **Export**: Download episode IDs for your training pipeline\n",
    "\n",
    "For more information, visit the [Tessera GitHub repository](https://github.com/arpitg1304/tessera)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}